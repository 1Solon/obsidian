{"path":".obsidian/plugins/text-extractor/cache/3ce2afafb3778379f3af6050fb1503e7.json","text":"Tree pruning » Decision trees have a natural tendency to segregate noisy data and create leaf nodes around these instances. » Opverfitting in a decision tree involves splitting data at an irrelevant feature. » The likelihood of over-fitting occurring increases as a tree gets deeper as predictions are based on smaller and smaller subsets * Pruning the tree identifies and removes sub-trees that are likely to be due to noise and sample variance — replace subtree with leaf node covering data partition at that point — may result in a tree not being consistent with training-data B‘EBI will promote generalisation. S~ :","libVersion":"0.2.2","langs":"eng"}