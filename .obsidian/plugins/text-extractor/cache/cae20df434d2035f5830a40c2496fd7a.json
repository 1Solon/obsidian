{"path":".obsidian/plugins/text-extractor/cache/cae20df434d2035f5830a40c2496fd7a.json","text":"¢ Asimple 1-NN classifier is easy to implement. But it will be susceptible to “noise” in the data. A misclassification will occur every time a single noisy example is retrieved. * We might decide to vary the neighbourhood size parameter k to improve the predictive performance of k-NN. * Choosing between different settings of an algorithm is often referred to as hyperparameter tuning or model selection. Rod o ®. o + Using a larger k (e.g. k > 2) can sometimes & “ ° make the classifier more robust and . ° ° \\q:\\ « But when ks large (k=N) and classes are o2e8 “\": O S unbalanced, we always predict the majority 0% % 0%06:8 : EBL” class. Foo o T","libVersion":"0.2.2","langs":"eng"}