{"path":".obsidian/plugins/text-extractor/cache/69d92a530e5d0dcd7c774c249f06dfa0.json","text":"* Preference Bias: Choose decision trees that have fewer tests, i.e. shallower trees * Pure nodes provide more information about the value of the target feature for a query. * Descriptive features that split the dataset into pure sets provide information about the target feature and are considered more informative. * Testing the informative features early on in the tree can result in shallower trees. * Claude Shannonâ€™s entropy model is a computational metric of the purity of a set. T","libVersion":"0.2.2","langs":"eng"}